**State:**  Agent 在环境中所处的一个状态；
**State Space:** 所有状态的集合 $\mathcal{S} = \{s_i\}$；
**Action**:  对于每个状态，Agent所能采取的动作，动作；
**Action Space of a state:**  对于某个状态，所能采取的所有动作的集合 $\mathcal{A} = \{a_i\}$；
**State transition:** 采取某个动作后，Agent的从 $s_i$ 到 $s_j$ ;
**Policy:** 一种策略，用来告诉Agent应该在某个状态 $s_i$ 下采取什么 $a_i$，可以用条件概率 $\pi(a_i | s_i)$ 表示；
**Reward:** 奖励，它是一个实数，用来表示当采取某个动作之后可得到的奖励，它可以看成是human-machine的交互接口，它可以使得人能够引导机器朝什么方向改进，可以用条件概率表示 $p(r|s,a)$, reward可以是确定性的，也可以是随机的，reward是基于当前的状态和动作，不是后面的状态和动作；
**Trajectory:** 是一个state-action-reward的链，$s_0,a_0,r_0,s_1,a_1,r_1,...,s_i,a_i,r_i,...$ ;
**Return:** 是关于trajectory的，它是整个过程中获得所有reward的和，可以用来评估哪个 policy 会更好；
**Discounted Return:** discount rate $\gamma \in (0, 1)$ , 它的作用 1、可以让无限长的trajectory的return是一个有限值；2、它可以平衡长远的reward还是近处的reward，当 $\gamma$ 趋近于0时，更多考虑近处的reward，趋近于 1 时，会考虑更长远的reward；
**Episode:** 但 Agent 与环境交互时，它的状态可能会在某个state下停止，从而产生的trajectory就称为episode，可以看成一个有限长的trajectory；

### 马尔可夫决策过程（MDP)
- 集合
	- **State：** state的集合 $\mathcal{S}$
	- **Action：** action的集合 $\mathcal{A}(s)$ 是与状态有关 $s \in \mathcal{S}$
	- **Reward：** reward的集合 $\mathcal{R}(s, a)$ 
- 概率分布
	- **State transition probability：** 某个状态 $s$ ，采取动作 $a$ ，转移到新状态的概率 $p(s'|s,a)$
	- **Reward probability：** 在某个状态 $s$ , 采取动作 $a$ , 获取回报 $r$ 的概率 $p(r|s,a)$
- **Policy：** 在某个状态 $s$ , 采取动作 $a$ 的概率 $\pi(a|s)$ 
- **Markov property：** 
   $$p(s_{t+1}|a_{t+1},s_t,...,a_1,s_0)=p(s_{t+1}|a_{t+1},s_t)$$
   $$p(r_{t+1}|a_{t+1},s_t,...,a_1,s_0)=p(r_{t+1}|a_{t+1},s_t)$$
