---
tags:
  - DiffusionModel
  - ProbabilityModels
title: 理解扩散模型
date: 2024-01-11 14:37
---

---

## **生成模型**

**目标：** 基于观察到的数据，构建一个模型，能尽可能真实地反映该类数据的概率分布情况，然后我们就能够以该概率分布通过采样的方式去生成这类数据。

但是对于观察到的数据，我们不会直接去同一个纬度上对他建模，在许多场景下，我们所观察到的世界可以由一组关联的但不可见的隐变量者表达。虽然我们无法穷举这些隐变量，因为我们根本不能认识这个真实的世界，但是我们可以通过提炼部分抽象特征去描述一个事物。


## Evidence Lower Bound(ELBO)

如果要优化似然函数，通常是最大化 ELBO，因为似然函数和 ELBO 有如下关系：

$$\log p(x) \geq \mathbb{E}_{q_{\phi}(z|x)}  \left[ \log \frac{p(x, z)}{q_{\phi}(z|x)} \right]$$
推导过程如下：

$$
\begin{align}
\log p(x) &= \log p(x) \int q_{\phi}(z|x)dz \\
&=\int q_{\phi}(z|x)(\log p(x))dz \\
&=\mathbb{E}_{q_{\phi}(z|x)} \left[ \log p(x) \right] \\
&=\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)}{p(z|x)} \right] \\
&=\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)q_{\phi}(z|x)}{p(z|x)q_{\phi}(z|x)} \right] \\
&=\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)}{q_{\phi}(z|x)} \right] + \mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{q_{\phi}(z|x)}{p(z|x)} \right] \\
&=\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)}{q_{\phi}(z|x)} \right] + D_{KL}(q_{\phi}(z|x) \|p(z|x)) \\
&\geq\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)}{q_{\phi}(z|x)} \right]
\end{align}
$$

因为 $\log p(x)$ 是一个常量，因此最大化 ELBO 实际上同时在最小化 KL 散度。

## VAE

![[Pasted image 20240124095716.png|200]]

变分自编码的优化其实就是最大化 ELBO 的过程，我们对 ELBO 进一步推导：

$$
\begin{align}
\mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(x,z)}{q_{\phi}(z|x)} \right] &= \mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(z)p_{\theta}(x|z)}{q_{\phi}(z|x)} \right] \\
&= \mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right] + \mathbb{E}_{q_{\phi}(z|x)} \left[ \log \frac{p(z)}{q_{\phi}(z|x)} \right] \\
&=\underbrace{\mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right]}_{重构部分} - \underbrace{D_{KL}(q_{\phi}(z|x) \| p(z))}_{先验匹配部分}
\end{align}
$$

可以看到 ELBO 包含了两部分内容，第一部分是重构部分，我们希望这部分的值最大化，我们对隐变量建模后，想要评价建模的质量好坏，就是要看隐变量是否可用，即他们是否能可靠地生成真实地数据。比如我们对狗建模，隐变量可能有{忠诚、人类朋友、跑得快}等特征，我们通过构建这些特征是否能让人第一时间对应到狗上。能够对应上就说明上好的特征，否则就说明构建的不到位。第二部分是先验匹配部分，这部分有助于编码器生成某种分布，而不是单一的某个值。

- **学习到的分布**：指的是变分自编码器（VAE）中通过编码器部分学习到的潜在空间分布。这个分布捕捉了输入数据的关键特征和结构。
- **建模有效的潜在变量**：意味着这个潜在空间的分布能够有效地表示输入数据的重要信息。换句话说，潜在变量包含了足够的信息来捕捉和描述输入数据的本质特征。
- **重建原始数据**：指的是通过解码器部分将潜在变量转换回原始输入数据。如果潜在空间分布有效地捕捉了输入数据的特征，解码器应该能够利用这些潜在变量准确地重建或生成与原始数据类似的数据。

## 多层级 VAE

![[Pasted image 20240124095558.png#pic_center | 400]]

联合概率以及后验概率的数学表达式如下：
$$p(x,z_{1:T}) = p(z_T)p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_t)$$
$$q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x)\prod_{t=2}^Tq_{\phi}(z_t|z_{t-1})$$
ELBO 改写后如下：

$$
\begin{align}
\log p(x) &=\log \int p(x,z_{1:T})dz_{1:T} \\
&= \log \int \frac{p(x,z_{1:T})q_{\phi}(z_{1:T}|x)}{q_{\phi}(z_{1:T}|x)}dz_{1:T} \\
&= \log \mathbb{E}_{q_{\phi}(z_{1:T}|x)} \left[ \frac {p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right]dz_{1:T} \\
&\geq \mathbb{E}_{q_{\phi}(z_{1:T}|x)} \log \left[ \frac {p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right] dz_{1:T} \\
&= \mathbb{E}_{q_{\phi}(z_{1:T}|x)} \left[ \log \frac {p(z_T)p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_t)} {q_{\phi}(z_1|x)\prod_{t=2}^Tq_{\phi}(z_t|z_{t-1})} \right]
\end{align}
$$

## 变分扩散模型（VDM）

![[Pasted image 20240124103620.png#pic_right|440]]

变分扩散模型最简单的理解是在 MHAE 的基础上，增加三个约束条件：

- 隐变量的维度与原始数据的维度一致（也就是说没有降维的过程）
- encoder $q(x_t|x_{t-1})$ 不是学习得到的，而是使用的是默认的预定义的线性高斯模型，只与前一刻的输出有关
- encoder 的高斯模型参数会发生变化，并让最后的输出结果服从标准正态分布

根据约束 1，在 VDM 中后验概率的数学表达为：
$$q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})$$
根据约束 2，在 VDM 中 encoder 的表达式如下：
$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)\mathrm{I})$$
根据约束 3，在 VDM 中的联合概率的数学表达式为：

$$
\begin{align}
p(x_{0:T}) &= p(x_T) \prod_{t=1}^T p_{\theta} (x_{t-1} | x_t) \\
\text{where,} & \\
p(x_T) &= \mathcal{N}(x_T; \mathrm{0},\mathrm{I})
\end{align}
$$

VDM 也可以通过最大化 ELBO 进行优化：

$$
\begin{align}
\log p(x) &= \log \int p(x_{0:T})dx_{1:T} \\
&= \log \int \frac {p(x_{0:T}) q(x_{1:T}|x_0)} {q(x_{1:T}|x_0)} dx_{1:T} \\
&= \log \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \frac {p(x_{0:T})} {q(x_{1:T}|x_0)} \right] \\
&\ge \mathbb{E}_{q(x_{1:T}|x_0)} \log \left[ \frac {p(x_{0:T})} {q(x_{1:T}|x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \log \left[ \frac {p(x_T) \prod_{t=1}^T p_{\theta} (x_{t-1} | x_t)} {\prod_{t=1}^T q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \log \left[ \frac {p(x_T) p_{\theta}(x_0|x_1) \prod_{t=2}^T p_{\theta} (x_{t-1} | x_t)} {q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \log \left[ \frac {p(x_T) p_{\theta}(x_0|x_1) \prod_{t=1}^T p_{\theta} (x_{t} | x_{t+1})} {q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_{\theta}(x_0|x_1)} {q(x_T|x_{T-1})} \right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \prod_{t=1}^{T-1} \frac{p_{\theta} (x_{t} | x_{t+1})} {q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log p_{\theta}(x_0|x_1)
\right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac { p(x_T) } { q(x_T|x_{T-1}) } \right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \sum_{t=1}^{T-1} \log \frac { p_{\theta} (x_{t} | x_{t+1}) } { q(x_t|x_{t-1}) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log p_{\theta}(x_0|x_1)
\right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac { p(x_T) } { q(x_T|x_{T-1}) } \right] + \sum_{t=1}^{T-1} \mathbb{E}_{q(x_{1:T}|x_0)} \left[  \log \frac { p_{\theta} (x_{t} | x_{t+1}) } { q(x_t|x_{t-1}) } \right] \\
&= \mathbb{E}_{q(x_{1}|x_0)} \left[ \log p_{\theta}(x_0|x_1)
\right] + \mathbb{E}_{q(x_{T}, x_{T-1}|x_0)} \left[ \log \frac { p(x_T) } { q(x_T|x_{T-1}) } \right]
+ \sum_{t=1}^{T-1} \mathbb{E}_{q(x_{t-1},x_t,x_{t+1}|x_0)} \left[  \log \frac { p_{\theta} (x_{t} | x_{t+1}) } { q(x_t|x_{t-1}) } \right] \\
&= \underbrace{ \mathbb{E}_{q(x_{1}|x_0)} \left[ \log p_{\theta}(x_0|x_1)
\right]}_{重构部分}
- \underbrace {\mathbb{E}_{q(x_{T-1}|x_0)} \left[ D_{KL} (q(x_T | x_{T-1}) || p(x_T)) \right]}_{先验匹配部分}
- \sum_{t=1}^{T-1} \underbrace{ \mathbb{E}_{q(x_{t-1},x_{t+1}|x_0)} \left[ D_{KL}(q(x_t|x_{t-1}) || p_{\theta}(x_t|x_{t+1})) \right]}_{一致性部分}


\end{align}
$$

对上面的三个部分解释如下：

1. 重构部分：在给定第一个 latent 预测原始数据的概率分布
2. 先验匹配部分：当最后一层的概率分布与标准正太分布一致时，数值最小，这部分内容不需要优化，没有需要训练的参数
3. 一致性部分：这部分对于 xt 的分布希望从前向和后向能满足一致性

对一致性的部分进行改进，让他只与一个变量相关，以减少方差，改进思路如下：
$$q(x_t|x_{t-1}) = q(x_t|x_{t-1},x_0)=\frac {q(x_{t-1}|x_t,x_0)q(x_t|x_0)} {q(x_{t-1}|x_0)}$$
则 ELBO 推导过程如下：
$$
\begin{align}
\log p(x) &\ge \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_{0:T}) } { q(x_{1:T}|x_0) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) \prod_{t=1}^T p_{\theta}(x_{t-1}|x_t) } { \prod_{t=1}^{T} q(x_t|x_{t-1}) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p(x_0|x_1) \prod_{t=2}^T p_{\theta}(x_{t-1}|x_t) } { q(x_1|x_0) \prod_{t=2}^{T}  q(x_t|x_{t-1}) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p(x_0|x_1) \prod_{t=2}^T p_{\theta}(x_{t-1}|x_t) } { q(x_1|x_0) \prod_{t=2}^{T}  q(x_t|x_{t-1},x_0) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { q(x_1|x_0)  } 
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  q(x_t|x_{t-1},x_0) } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { q(x_1|x_0)  } 
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  \frac {q(x_{t-1}|x_t,x_0)q(x_t|x_0)} {q(x_{t-1}|x_0)} } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { q(x_1|x_0)  } 
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  \frac {q(x_{t-1}|x_t,x_0)q(x_t|x_0)} {q(x_{t-1}|x_0)} } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { q(x_1|x_0)  } 
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  \frac {q(x_{t-1}|x_t,x_0)q(x_t|x_0)} {q(x_{t-1}|x_0)} } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { q(x_1|x_0)  } 
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  \frac {q(x_{t-1}|x_t,x_0) \cancel{q(x_t|x_0)}} {\cancel{q(x_{t-1}|x_0)}} } \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } { \cancel { q(x_1|x_0) }  }  + \log \frac { \cancel {q(x_1|x_0)} } { q(x_T|x_0) }
+ \log \prod_{t=2}^T \frac {  p_{\theta}(x_{t-1}|x_t) } {  q(x_{t-1}|x_t,x_0) }  \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p(x_T) p_{\theta}(x_0|x_1)  } {  q(x_T|x_0)   } 
+ \sum_{t=2}^T \log  \frac {  p_{\theta}(x_{t-1}|x_t) } {  q(x_{t-1}|x_t,x_0) }  \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log p_{\theta}(x_0|x_1) \right] 
+ \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac { p(x_T) } { q(x_T|x_0) } \right] 
+ \sum_{t=2}^T \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{ p_{\theta}(x_{t-1}|x_t) } { q(x_{t-1}|x_t,x_0) } \right] \\
&= \mathbb{E}_{q(x_{1}|x_0)} \left[ \log p_{\theta}(x_0|x_1) \right]
+ \mathbb{E}_{q(x_{T}|x_0)} \left[ \log \frac { p(x_T) } { q(x_T|x_0) } \right]
+ \sum_{t=2}^T \mathbb{E}_{q(x_{t},x_{t-1}|x_0)} \left[ \log \frac{ p_{\theta}(x_{t-1}|x_t) } { q(x_{t-1}|x_t,x_0) } \right] \\
&= \underbrace{\mathbb{E}_{q(x_{1}|x_0)} \left[ \log p_{\theta}(x_0|x_1) \right]}_{重构部分} 
- \underbrace {D_{KL}(q(x_T|x_0)||p(x_T))}_{先验匹配区域} 
	- \sum_{t=2}^T \underbrace { \mathbb{E}_{q(x_t|x_0)} \left[ D_{KL}(q(x_{t-1}|x_t,x_0) || p_{\theta}(x_{t-1}|x_t)) \right] }_{去燥匹配部分}
\end{align} 
$$

去燥匹配部分的优化思路

$$ q(x_{t-1}|x_t,x_0) = \frac {q(x_t | x_{t-1},x_0)q(x_{t-1}|x_0)} {q(x_t | x_0)} $$

$q(x_t | x_{t-1},x_0) = q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t) \mathrm{I})$ 基于马尔可夫的性质。

$x_t \sim q(x_t|x_{t-1})$ 根据重参数化的规则可以重写成：

$$
\begin{align}
x_t &= \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon^{*}_{t-1} \\
&= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon_{t-2}^{*} ) + \sqrt{1-\alpha_t}\epsilon^*_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{\alpha_t - \alpha_t\alpha_{t-1}}\epsilon_{t-2}^* + \sqrt{1-\alpha_t}\epsilon^*_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{\sqrt{\alpha_t - \alpha_t\alpha_{t-1}}^2 + \sqrt{1-\alpha_t}^2}\epsilon_{t-2} \\
&= \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2} \\
&= \: ... \\
&= \sqrt{\prod_{i=1}^t\alpha_i}x_0 + \sqrt{1-\prod_{i=1}^t\alpha_i}\epsilon_0 \\
&= \sqrt{\overline{\alpha}_t}x_0 + \sqrt{1-\overline{\alpha}_t}\epsilon_0 \\
&\sim \mathcal{N}(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)\mathrm{I})
\end{align}
$$






