---
tags:
  - DeepRL
title: 贝尔曼价值函数公式推导
date: 2025-02-08 10:24
type:
---
---
## 贝尔曼公式推导流程
1、价值函数公式
$$
V^\pi(s) = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^kr_{t+k} | s_t = s\right]
$$

2、累计回报函数拆分
$$
G_t=\sum_{k=0}^{\infty}\gamma^kr_{t+k}=r_t+\gamma\sum_{k=1}^\infty\gamma^{k-1}r_{t+k} = r_t + \gamma\sum_{k=0}^\infty\gamma^kr_{t+1+k}=r_t+\gamma G_{t+1}
$$
3、代入价值函数
$$
V^\pi(s)=\mathbb{E}_\pi\left[r_t+\gamma G_{t+1}\right|s_t=s]=\mathbb{E}_\pi\left[r_t|s_t=s\right] + \gamma\mathbb{E}_\pi\left[G_{t+1}|s_t=s\right]
$$
4、即时奖励期望
$$
r_t=R(s,a,s')
$$
其中：

• $R(s, a, s')$ ：是奖励函数，表示在状态  $s$  执行动作  $a$  后转移到  $s'$  时的奖励。
由于动作 $a$ 是由 $\pi(s, a)$ 决定的，且下一步状态 $s'$ 是由状态转移函数 $T(s,a,s')$ 决定的，因此需要对所有可能的动作 $a$ 以及 $s'$ 求期望。

因此引入策略 $\pi$ 后的即时期望：
$$
\mathbb{E}_\pi\left[r_t|s_t=s\right] = \sum_{a \in A}\pi(s,a)\mathbb{E}_{s'\sim T(s,a,\cdot)}\left[R(s,a,s')\right] = \sum_{a \in A}\pi(s,a)\sum_{s' \in S}T(s,a,s')R(s,a,s')
$$
 5、递归奖励期望
 $$
\mathbb{E}_\pi\left[\gamma G_{t+1}\right | s_t=s] =  \sum_{a \in A}\pi(s,a)\sum_{s' \in S}T(s,a,s')V^\pi(s')
$$
6、结合两部分内容
$$
V^\pi(s) = \sum_{a \in A}\pi(s,a)\sum_{s' \in S}T(s,a,s')(R(s,a,s') + \gamma V^\pi(s'))
$$
**直观理解：**
这一步的本质是：

1. 当前状态的价值  $V^\pi(s)$  由两个部分组成：

• 当前即时奖励的期望  $R(s, a, s')$ 。

• 未来状态价值的折扣和  $\gamma V^\pi(s')$ 。

2. 动作选择的概率由策略  $\pi(s, a)$ 给出，未来状态的转移概率由  $T(s, a, s')$  给出。


因此，通过对所有可能的动作  a  和状态  $s'$  求加权平均，得到当前状态的价值。

### 策略的预期回报
$$
V^\pi(s) = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^kr_{t+k} | s_t = s\right]
$$
根据上式有：
$$
V^\pi(s_0) = \int_S \rho^\pi(s)\int_A \pi(s,a)R'(s,a) \mathrm{d}a \mathrm{d}s=\int_S \rho^\pi(s)\int_A \pi(s,a)\int_{s'\in S}T(s,a,s')R(s,a,s')\mathrm{d}s'\mathrm{d}a\mathrm{d}s
$$
对于一个可微分的 policy $\pi_{\omega}$ 有：
$$
V^{\pi_{\omega}}(s_0) = \int_S \rho^{\pi_\omega}(s)\int_A \pi_{\omega}(s,a)R'(s,a) \mathrm{d}a \mathrm{d}s= \int_S \rho^{\pi_\omega}(s)\int_A \pi_{\omega}(s,a)Q^{\pi_\omega}(s,a)\mathrm{d}a\mathrm{d}s
$$